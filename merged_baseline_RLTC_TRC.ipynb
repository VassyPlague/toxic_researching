{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b819128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14be061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('Russian Language Toxic Comments/Russian Language Toxic Comments.csv')\n",
    "data2 = pd.read_csv('Toxic Russian Comments/Toxic Russian Comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2cf1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>–í —à–∞–ø–∫–µ –±—ã–ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–Ω—Ñ—É –ø–æ —Ç–µ–∫—É—â–µ–º—É —Ñ–∏–ª—å–º—É...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>–ü–æ—á–∏—Ç–∞–π—Ç–µ –ø–æ—Å—Ç—ã —É —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä–∞,–º–æ–∂–µ—Ç –Ω–∞–π–¥–µ—Ç–µ —á...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>–ü—Ä–æ –≥—Ä–∞—Ñ–∏–∫—É –±—ã–ª–æ –æ–±–∏–¥–Ω–æ) —è —Ç–∞–∫ —Ç–æ –ø—Ä–æ—Ö–æ–¥–∏–ª –≤—Å–µ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https: pp.userapi.com c848520 v848520411 11627...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>–í–æ–∑—å–º—ë–º –∫–∞–∫ –ø—Ä–∏–º–µ—Ä –†–æ—Å—Å–∏—é, –∑–∞–ø–∞–¥–Ω–æ–µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8305</th>\n",
       "      <td>–∞ —Ç–µ–ø–µ—Ä—å –≤—Å–ø–æ–º–Ω–∏ –∫–∞–∫–∏–µ –æ—â—É—â–µ–Ω–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç —Å–ª—É—á–∞...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8306</th>\n",
       "      <td>–ù–µ –ø–æ–π–º—É, –∞ –≥–¥–µ –º–æ—Ç–æ—Ä –Ω–∞ —ç–ª–µ–∫—Ç—Ä–æ–≤–µ–ª–∏–∫–µ? –ò–ª–∏ –æ–Ω...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8307</th>\n",
       "      <td>–î–∞, —ç—Ç–æ –æ–±—Ä–∞—Ç–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ –º–µ–¥–∞–ª–∏. –ü–æ —ç—Ç–æ–º—É –∏ —Å—Ç...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8309</th>\n",
       "      <td>–û –¥–∞! –î–µ–ª–∞—é –∏–Ω–æ–≥–¥–∞ —Ç–∞–∫–æ–π, –æ—á–µ–Ω—å –≤–∫—É—Å–Ω—ã–π.\\n</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8311</th>\n",
       "      <td>–ù–µ –Ω–∞—Å—Ç–æ–ª—å–∫–æ –≤—Å–µ –ø–ª–æ—Ö–æ —Å —Ä–∞–π–æ–Ω–æ–º –û—Ç —Ü–µ–Ω—Ç—Ä–∞ –º–∏–Ω...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  toxic\n",
       "6     –í —à–∞–ø–∫–µ –±—ã–ª–∏ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏–Ω—Ñ—É –ø–æ —Ç–µ–∫—É—â–µ–º—É —Ñ–∏–ª—å–º—É...    0.0\n",
       "12    –ü–æ—á–∏—Ç–∞–π—Ç–µ –ø–æ—Å—Ç—ã —É —ç—Ç–æ–≥–æ –∞–≤—Ç–æ—Ä–∞,–º–æ–∂–µ—Ç –Ω–∞–π–¥–µ—Ç–µ —á...    0.0\n",
       "17    –ü—Ä–æ –≥—Ä–∞—Ñ–∏–∫—É –±—ã–ª–æ –æ–±–∏–¥–Ω–æ) —è —Ç–∞–∫ —Ç–æ –ø—Ä–æ—Ö–æ–¥–∏–ª –≤—Å–µ...    0.0\n",
       "28    https: pp.userapi.com c848520 v848520411 11627...    0.0\n",
       "35    –í–æ–∑—å–º—ë–º –∫–∞–∫ –ø—Ä–∏–º–µ—Ä –†–æ—Å—Å–∏—é, –∑–∞–ø–∞–¥–Ω–æ–µ–≤—Ä–æ–ø–µ–π—Å–∫–∏–µ ...    0.0\n",
       "...                                                 ...    ...\n",
       "8305  –∞ —Ç–µ–ø–µ—Ä—å –≤—Å–ø–æ–º–Ω–∏ –∫–∞–∫–∏–µ –æ—â—É—â–µ–Ω–∏—è –≤—ã–∑—ã–≤–∞–µ—Ç —Å–ª—É—á–∞...    0.0\n",
       "8306  –ù–µ –ø–æ–π–º—É, –∞ –≥–¥–µ –º–æ—Ç–æ—Ä –Ω–∞ —ç–ª–µ–∫—Ç—Ä–æ–≤–µ–ª–∏–∫–µ? –ò–ª–∏ –æ–Ω...    0.0\n",
       "8307  –î–∞, —ç—Ç–æ –æ–±—Ä–∞—Ç–Ω–∞—è —Å—Ç–æ—Ä–æ–Ω–∞ –º–µ–¥–∞–ª–∏. –ü–æ —ç—Ç–æ–º—É –∏ —Å—Ç...    0.0\n",
       "8309         –û –¥–∞! –î–µ–ª–∞—é –∏–Ω–æ–≥–¥–∞ —Ç–∞–∫–æ–π, –æ—á–µ–Ω—å –≤–∫—É—Å–Ω—ã–π.\\n    0.0\n",
       "8311  –ù–µ –Ω–∞—Å—Ç–æ–ª—å–∫–æ –≤—Å–µ –ø–ª–æ—Ö–æ —Å —Ä–∞–π–æ–Ω–æ–º –û—Ç —Ü–µ–Ω—Ç—Ä–∞ –º–∏–Ω...    0.0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLTC_toxic = data1[data1['toxic'] == float(1)]\n",
    "RLTC_not_toxic = data1[data1['toxic'] == float(0)]\n",
    "RLTC_not_toxic = RLTC_not_toxic[:5000]\n",
    "RLTC_not_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9bd57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\513232936.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TRC_toxic.rename(columns = {'text' : 'comment', 'label' : 'toxic'}, inplace= True)\n"
     ]
    }
   ],
   "source": [
    "TRC_toxic = data2[data2['label'] == 1]\n",
    "TRC_no_toxic = data2[data2['label'] == 0]\n",
    "TRC_no_toxic = TRC_no_toxic[:25000]\n",
    "TRC_no_toxic = TRC_no_toxic[:25000]\n",
    "TRC_toxic.rename(columns = {'text' : 'comment', 'label' : 'toxic'}, inplace= True)\n",
    "TRC_no_toxic.rename(columns = {'text' : 'comment', 'label' : 'toxic'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca7f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\75253641.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  toxic = RLTC_toxic.append(TRC_toxic, ignore_index = True)\n",
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\75253641.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  no_toxic = RLTC_not_toxic.append(TRC_no_toxic, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "toxic = RLTC_toxic.append(TRC_toxic, ignore_index = True)\n",
    "no_toxic = RLTC_not_toxic.append(TRC_no_toxic, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d2f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\38378708.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = toxic.append(no_toxic, ignore_index= True)\n"
     ]
    }
   ],
   "source": [
    "data = toxic.append(no_toxic, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0de605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b9dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['toxic'] = data['toxic'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ded5e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>–ª–µ–Ω—è —Å–Ω–∏–º–∞–π –∫–æ–º–∞–Ω–¥—É, –±—É–¥—å –º—É–∂–∏–∫–æ–º, —Å–∫–∞–∑–∞–ª - —Å–¥...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>—Ç–≤–æ—è —Å—Ç—Ä–∞–Ω–∞ —Ö–æ—Ö–ª—è–Ω–¥–∏—è —É–º–Ω—ã–π –Ω–∏–∫–æ–≥–¥–∞ –∏ –Ω–µ –±—ã–ª–∞....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>—Ç—ã –ø—Ä–∞–≤, —Ç—ã –∂–µ —Å–∞–º–∞ —Ä—É—Å—Å–∫–∞—è —Ç–≤–∞—Ä—å, –Ω–µ —Å—É–¥–∏ —Ä—É—Å...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–µ—Å–ª–∏ –º—ã —Å–∞–º–∏ —ç—Ç–∏–º –ø–∏–¥–æ—Ä–∞–º –Ω–µ —Å–∫–∞–∂–µ–º –Ω–µ—Ç —Ç–æ —Ç–∞–∫...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–≤–æ—Ç –º—É–∂–∏–∫—É, –∏ –∑–∞–¥—É–º–∞—Ç—å—Å—è,—á—Ç–æ –Ω–µ —Ç–∞–∫ –∏ –ø–æ—á–µ–º—É,,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59539</th>\n",
       "      <td>–∫–∞–∫–æ–π –¥–æ–ª–±–æ–µ–± –ø–∏—à–µ—Ç –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–ª–∞—Å—Å? –ø–æ–∫–∞–∂–∏—Ç–µ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59540</th>\n",
       "      <td>–Ω–µ—Ç –≤ –≥–∏–±–∏—Å–∫—É—Å–µ —Å–æ–∑–¥–∞—é—Ç—Å—è,–∫—É—Å—Ç—ã —Å –±–∞–±–æ—á–∫–∞–∏ –∏—Ö ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59541</th>\n",
       "      <td>—Ç–∞–∫ —ç—Ç–∏—Ö —Å—É—á–∞—Ä –∏ –Ω–∞–¥–æ –ø–∏–∑–¥–∏—Ç—å! —á—Ç–æ–±—ã –¥–µ–Ω–µ–∂–∫—É –∫...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59542</th>\n",
       "      <td>–∞ –±–∞–Ω–∫–∏ —Ç–æ–∂–µ –æ—Ç–¥–∞–µ—Ç–µ?–∏–ª–∏ –∏–º –º–µ—Å—Ç–æ –Ω–∞–π–¥–µ—Ç—Å—è –≤ –Ω...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59543</th>\n",
       "      <td>üòÜüòÜüòÜ –∞ —è –∏ –Ω–µ –≥–æ–≤–æ—Ä—é —á—Ç–æ —Ä—É—Å—Å–∫–∏–µ –æ–¥–Ω–∏ –æ—Å–æ–±–µ–Ω–Ω—ã–µ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59544 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic\n",
       "0      –ª–µ–Ω—è —Å–Ω–∏–º–∞–π –∫–æ–º–∞–Ω–¥—É, –±—É–¥—å –º—É–∂–∏–∫–æ–º, —Å–∫–∞–∑–∞–ª - —Å–¥...      1\n",
       "1      —Ç–≤–æ—è —Å—Ç—Ä–∞–Ω–∞ —Ö–æ—Ö–ª—è–Ω–¥–∏—è —É–º–Ω—ã–π –Ω–∏–∫–æ–≥–¥–∞ –∏ –Ω–µ –±—ã–ª–∞....      0\n",
       "2      —Ç—ã –ø—Ä–∞–≤, —Ç—ã –∂–µ —Å–∞–º–∞ —Ä—É—Å—Å–∫–∞—è —Ç–≤–∞—Ä—å, –Ω–µ —Å—É–¥–∏ —Ä—É—Å...      1\n",
       "3      –µ—Å–ª–∏ –º—ã —Å–∞–º–∏ —ç—Ç–∏–º –ø–∏–¥–æ—Ä–∞–º –Ω–µ —Å–∫–∞–∂–µ–º –Ω–µ—Ç —Ç–æ —Ç–∞–∫...      1\n",
       "4      –≤–æ—Ç –º—É–∂–∏–∫—É, –∏ –∑–∞–¥—É–º–∞—Ç—å—Å—è,—á—Ç–æ –Ω–µ —Ç–∞–∫ –∏ –ø–æ—á–µ–º—É,,...      0\n",
       "...                                                  ...    ...\n",
       "59539  –∫–∞–∫–æ–π –¥–æ–ª–±–æ–µ–± –ø–∏—à–µ—Ç –ø–æ—Å—Ç–∞–≤–∏—Ç—å –∫–ª–∞—Å—Å? –ø–æ–∫–∞–∂–∏—Ç–µ ...      1\n",
       "59540  –Ω–µ—Ç –≤ –≥–∏–±–∏—Å–∫—É—Å–µ —Å–æ–∑–¥–∞—é—Ç—Å—è,–∫—É—Å—Ç—ã —Å –±–∞–±–æ—á–∫–∞–∏ –∏—Ö ...      0\n",
       "59541  —Ç–∞–∫ —ç—Ç–∏—Ö —Å—É—á–∞—Ä –∏ –Ω–∞–¥–æ –ø–∏–∑–¥–∏—Ç—å! —á—Ç–æ–±—ã –¥–µ–Ω–µ–∂–∫—É –∫...      1\n",
       "59542  –∞ –±–∞–Ω–∫–∏ —Ç–æ–∂–µ –æ—Ç–¥–∞–µ—Ç–µ?–∏–ª–∏ –∏–º –º–µ—Å—Ç–æ –Ω–∞–π–¥–µ—Ç—Å—è –≤ –Ω...      0\n",
       "59543  üòÜüòÜüòÜ –∞ —è –∏ –Ω–µ –≥–æ–≤–æ—Ä—é —á—Ç–æ —Ä—É—Å—Å–∫–∏–µ –æ–¥–Ω–∏ –æ—Å–æ–±–µ–Ω–Ω—ã–µ...      0\n",
       "\n",
       "[59544 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea9da2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30000\n",
       "1    29544\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4091a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11254118",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['comment'], data['toxic'], random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d393f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_len_sort(corpus, labels):\n",
    "    output_corpus = []\n",
    "    output_labels = []\n",
    "    for index, doc in enumerate(corpus):\n",
    "        if len(doc.split()) > 10:\n",
    "            output_corpus.append(doc)\n",
    "            output_labels.append(labels[index])\n",
    "    return output_corpus, output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f929523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = doc_len_sort(list(X_train), list(y_train))\n",
    "X_test, y_test = doc_len_sort(list(X_test), list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8921d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31635"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c9b6d",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b273c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import numpy as np\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "#ALSO TRY SPACY LEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63156096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "977349de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_sw = stopwords.words(\"russian\") # ru stop words\n",
    "snowball = SnowballStemmer(language = \"russian\")# stemming\n",
    "#porter = PorterStemmer()\n",
    "#wordnet = WordNetLemmatizer()\n",
    "\n",
    "def token_all_proccesing(df: str, remove_stop_words: bool = True): # func\n",
    "    tokens = word_tokenize(df, language=\"russian\") # tokenize\n",
    "    tokens = [i for i in tokens if i not in string.punctuation] # tokenize w/o punctuation\n",
    "    if remove_stop_words:\n",
    "        tokens = [i for i in tokens if i not in ru_sw]# remove ru stopword\n",
    "    #tokens = [wordnet.stem(i) for i in tokens]\n",
    "    #tokens = [wordnet.lemmatize(i) for i in tokens] # lemmatize\n",
    "    tokens = [snowball.stem(i) for i in tokens] # snowball stemming\n",
    "    #tokens = [porter.stem(i) for i in tokens] # porter stemmming\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b135b",
   "metadata": {},
   "source": [
    "# Count Vectorizer and Log. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c027ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_CV_LOGREG = Pipeline([(\"vectorizer\",\n",
    "                            CountVectorizer(ngram_range=(1,3),binary = True ,tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True)))\n",
    "                  ,(\"model\", LogisticRegression(random_state = 228) )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13541827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(binary=True, ngram_range=(1, 3),\n",
       "                                 tokenizer=<function <lambda> at 0x000001DAE061B820>)),\n",
       "                ('model', LogisticRegression(random_state=228))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline_CV_LOGREG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b61acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_CV = model_pipeline_CV_LOGREG.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cce73",
   "metadata": {},
   "source": [
    "# TF-IDF and Log. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1450829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_TFIDF_LOGREG = Pipeline([(\"vectorizer\",TfidfVectorizer(tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True)))\n",
    "                  ,(\"model\",LogisticRegression(random_state = 228) )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15d25ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function <lambda> at 0x000001DAE239E430>)),\n",
       "                ('model', LogisticRegression(random_state=228))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline_TFIDF_LOGREG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51af63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tfidf = model_pipeline_TFIDF_LOGREG.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe414c3",
   "metadata": {},
   "source": [
    "# Count Vectorizer and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd8b6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary = True) # implies that it indicates whether the word is present or not.\n",
    "cv.fit(data['comment']) # find all the unique words from the training set\n",
    "train_x = cv.transform(X_train)\n",
    "test_x = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08d069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'eta': 0.75,\n",
    "         'max_depth': 150, # 100 88%\n",
    "         'n_estimators' : 500, # 100 88%\n",
    "         'objective' : 'binary:logitraw',\n",
    "         'reg' : 'logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bed8c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(train_x,y_train)\n",
    "xgb_test = xgb.DMatrix(test_x, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce25d9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:09:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"reg\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:09:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model_cv = xgb.train(param, xgb_train, num_boost_round = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "982de507",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost = xgb_model_cv.predict(xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76598603",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost = np.where(np.array(predict_xgboost) > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dcb5f7",
   "metadata": {},
   "source": [
    "# TF-IDF and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cd945ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "549d2adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Tfidf_vect = TfidfVectorizer(tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True))\n",
    "Tfidf_vect.fit(X_train, y_train)\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f333709",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b23997f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto', kernel='linear')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.fit(Train_X_Tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20814d",
   "metadata": {},
   "source": [
    "# TF-IDF and XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43333808",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd79428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function <lambda> at 0x000001DAEA490550>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.fit(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a599caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'eta': 0.75,\n",
    "         'max_depth': 150, # 100 88%\n",
    "         'n_estimators' : 500, # 100 88%\n",
    "         'objective' : 'binary:logitraw',\n",
    "         'reg' : 'logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac4e30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tf = tf_idf.transform(X_train)\n",
    "test_x_tf = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9aef1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_tf_idf = xgb.DMatrix(train_x_tf,y_train)\n",
    "xgb_test_tf_idf = xgb.DMatrix(test_x_tf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98400eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:15:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"reg\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:15:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model_tf_idf = xgb.train(param, xgb_train_tf_idf, num_boost_round = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca606b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost_tf_idf = xgb_model_tf_idf.predict(xgb_test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d2b4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost_tf_idf = np.where(np.array(predict_xgboost_tf_idf) > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "067e7cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_xgboost_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f55582",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a6afe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfb75fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + CountVectorzer Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      5686\n",
      "           1       0.85      0.93      0.89      4796\n",
      "\n",
      "    accuracy                           0.89     10482\n",
      "   macro avg       0.89      0.90      0.89     10482\n",
      "weighted avg       0.90      0.89      0.89     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Logistic Regression + CountVectorzer Metrics:')\n",
    "print (classification_report(predict_CV, y_test))\n",
    "## 25/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567979e",
   "metadata": {},
   "source": [
    "## metrics (accuracy) increases by ~ 5%\n",
    "with 25k/25k TRC and 5k/5k RLTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cb2b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + TF-IDF Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89      5607\n",
      "           1       0.86      0.92      0.89      4875\n",
      "\n",
      "    accuracy                           0.89     10482\n",
      "   macro avg       0.89      0.89      0.89     10482\n",
      "weighted avg       0.89      0.89      0.89     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Logistic Regression + TF-IDF Metrics:')\n",
    "print (classification_report(predict_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "533d1125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost + CountVectorzer Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86      6000\n",
      "           1       0.78      0.90      0.84      4482\n",
      "\n",
      "    accuracy                           0.85     10482\n",
      "   macro avg       0.85      0.86      0.85     10482\n",
      "weighted avg       0.86      0.85      0.85     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('XGBoost + CountVectorzer Metrics:')\n",
    "print (classification_report(predict_xgboost, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d08c6194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost + TF-IDF Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89      5832\n",
      "           1       0.83      0.93      0.87      4650\n",
      "\n",
      "    accuracy                           0.88     10482\n",
      "   macro avg       0.88      0.89      0.88     10482\n",
      "weighted avg       0.89      0.88      0.88     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('XGBoost + TF-IDF Metrics:')\n",
    "print (classification_report(predict_xgboost_tf_idf, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
