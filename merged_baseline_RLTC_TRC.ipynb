{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b819128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14be061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('Russian Language Toxic Comments/Russian Language Toxic Comments.csv')\n",
    "data2 = pd.read_csv('Toxic Russian Comments/Toxic Russian Comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2cf1d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>В шапке были ссылки на инфу по текущему фильму...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Почитайте посты у этого автора,может найдете ч...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Про графику было обидно) я так то проходил все...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https: pp.userapi.com c848520 v848520411 11627...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Возьмём как пример Россию, западноевропейские ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8305</th>\n",
       "      <td>а теперь вспомни какие ощущения вызывает случа...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8306</th>\n",
       "      <td>Не пойму, а где мотор на электровелике? Или он...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8307</th>\n",
       "      <td>Да, это обратная сторона медали. По этому и ст...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8309</th>\n",
       "      <td>О да! Делаю иногда такой, очень вкусный.\\n</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8311</th>\n",
       "      <td>Не настолько все плохо с районом От центра мин...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  toxic\n",
       "6     В шапке были ссылки на инфу по текущему фильму...    0.0\n",
       "12    Почитайте посты у этого автора,может найдете ч...    0.0\n",
       "17    Про графику было обидно) я так то проходил все...    0.0\n",
       "28    https: pp.userapi.com c848520 v848520411 11627...    0.0\n",
       "35    Возьмём как пример Россию, западноевропейские ...    0.0\n",
       "...                                                 ...    ...\n",
       "8305  а теперь вспомни какие ощущения вызывает случа...    0.0\n",
       "8306  Не пойму, а где мотор на электровелике? Или он...    0.0\n",
       "8307  Да, это обратная сторона медали. По этому и ст...    0.0\n",
       "8309         О да! Делаю иногда такой, очень вкусный.\\n    0.0\n",
       "8311  Не настолько все плохо с районом От центра мин...    0.0\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLTC_toxic = data1[data1['toxic'] == float(1)]\n",
    "RLTC_not_toxic = data1[data1['toxic'] == float(0)]\n",
    "RLTC_not_toxic = RLTC_not_toxic[:5000]\n",
    "RLTC_not_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9bd57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\513232936.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  TRC_toxic.rename(columns = {'text' : 'comment', 'label' : 'toxic'}, inplace= True)\n"
     ]
    }
   ],
   "source": [
    "TRC_toxic = data2[data2['label'] == 1]\n",
    "TRC_no_toxic = data2[data2['label'] == 0]\n",
    "TRC_no_toxic = TRC_no_toxic[:25000]\n",
    "TRC_no_toxic = TRC_no_toxic[:25000]\n",
    "TRC_toxic.rename(columns = {'text' : 'comment', 'label' : 'toxic'}, inplace= True)\n",
    "TRC_no_toxic.rename(columns = {'text' : 'comment', 'label' : 'toxic'}, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dca7f2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\75253641.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  toxic = RLTC_toxic.append(TRC_toxic, ignore_index = True)\n",
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\75253641.py:2: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  no_toxic = RLTC_not_toxic.append(TRC_no_toxic, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "toxic = RLTC_toxic.append(TRC_toxic, ignore_index = True)\n",
    "no_toxic = RLTC_not_toxic.append(TRC_no_toxic, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6d2f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\AppData\\Local\\Temp\\ipykernel_16348\\38378708.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = toxic.append(no_toxic, ignore_index= True)\n"
     ]
    }
   ],
   "source": [
    "data = toxic.append(no_toxic, ignore_index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0de605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sample(frac=1)\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b9dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['toxic'] = data['toxic'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ded5e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>леня снимай команду, будь мужиком, сказал - сд...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>твоя страна хохляндия умный никогда и не была....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ты прав, ты же сама русская тварь, не суди рус...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>если мы сами этим пидорам не скажем нет то так...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>вот мужику, и задуматься,что не так и почему,,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59539</th>\n",
       "      <td>какой долбоеб пишет поставить класс? покажите ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59540</th>\n",
       "      <td>нет в гибискусе создаются,кусты с бабочкаи их ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59541</th>\n",
       "      <td>так этих сучар и надо пиздить! чтобы денежку к...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59542</th>\n",
       "      <td>а банки тоже отдаете?или им место найдется в н...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59543</th>\n",
       "      <td>😆😆😆 а я и не говорю что русские одни особенные...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59544 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment  toxic\n",
       "0      леня снимай команду, будь мужиком, сказал - сд...      1\n",
       "1      твоя страна хохляндия умный никогда и не была....      0\n",
       "2      ты прав, ты же сама русская тварь, не суди рус...      1\n",
       "3      если мы сами этим пидорам не скажем нет то так...      1\n",
       "4      вот мужику, и задуматься,что не так и почему,,...      0\n",
       "...                                                  ...    ...\n",
       "59539  какой долбоеб пишет поставить класс? покажите ...      1\n",
       "59540  нет в гибискусе создаются,кусты с бабочкаи их ...      0\n",
       "59541  так этих сучар и надо пиздить! чтобы денежку к...      1\n",
       "59542  а банки тоже отдаете?или им место найдется в н...      0\n",
       "59543  😆😆😆 а я и не говорю что русские одни особенные...      0\n",
       "\n",
       "[59544 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea9da2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    30000\n",
       "1    29544\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4091a85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11254118",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['comment'], data['toxic'], random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95d393f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_len_sort(corpus, labels):\n",
    "    output_corpus = []\n",
    "    output_labels = []\n",
    "    for index, doc in enumerate(corpus):\n",
    "        if len(doc.split()) > 10:\n",
    "            output_corpus.append(doc)\n",
    "            output_labels.append(labels[index])\n",
    "    return output_corpus, output_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f929523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = doc_len_sort(list(X_train), list(y_train))\n",
    "X_test, y_test = doc_len_sort(list(X_test), list(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8921d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31635"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c9b6d",
   "metadata": {},
   "source": [
    "# Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b273c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vnvof\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "import numpy as np\n",
    "#from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "#ALSO TRY SPACY LEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63156096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "977349de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ru_sw = stopwords.words(\"russian\") # ru stop words\n",
    "snowball = SnowballStemmer(language = \"russian\")# stemming\n",
    "#porter = PorterStemmer()\n",
    "#wordnet = WordNetLemmatizer()\n",
    "\n",
    "def token_all_proccesing(df: str, remove_stop_words: bool = True): # func\n",
    "    tokens = word_tokenize(df, language=\"russian\") # tokenize\n",
    "    tokens = [i for i in tokens if i not in string.punctuation] # tokenize w/o punctuation\n",
    "    if remove_stop_words:\n",
    "        tokens = [i for i in tokens if i not in ru_sw]# remove ru stopword\n",
    "    #tokens = [wordnet.stem(i) for i in tokens]\n",
    "    #tokens = [wordnet.lemmatize(i) for i in tokens] # lemmatize\n",
    "    tokens = [snowball.stem(i) for i in tokens] # snowball stemming\n",
    "    #tokens = [porter.stem(i) for i in tokens] # porter stemmming\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b135b",
   "metadata": {},
   "source": [
    "# Count Vectorizer and Log. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c027ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_CV_LOGREG = Pipeline([(\"vectorizer\",\n",
    "                            CountVectorizer(ngram_range=(1,3),binary = True ,tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True)))\n",
    "                  ,(\"model\", LogisticRegression(random_state = 228) )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13541827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(binary=True, ngram_range=(1, 3),\n",
       "                                 tokenizer=<function <lambda> at 0x000001DAE061B820>)),\n",
       "                ('model', LogisticRegression(random_state=228))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline_CV_LOGREG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b61acd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_CV = model_pipeline_CV_LOGREG.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9cce73",
   "metadata": {},
   "source": [
    "# TF-IDF and Log. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1450829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_TFIDF_LOGREG = Pipeline([(\"vectorizer\",TfidfVectorizer(tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True)))\n",
    "                  ,(\"model\",LogisticRegression(random_state = 228) )])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15d25ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(tokenizer=<function <lambda> at 0x000001DAE239E430>)),\n",
       "                ('model', LogisticRegression(random_state=228))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pipeline_TFIDF_LOGREG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51af63f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tfidf = model_pipeline_TFIDF_LOGREG.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe414c3",
   "metadata": {},
   "source": [
    "# Count Vectorizer and XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd8b6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(binary = True) # implies that it indicates whether the word is present or not.\n",
    "cv.fit(data['comment']) # find all the unique words from the training set\n",
    "train_x = cv.transform(X_train)\n",
    "test_x = cv.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08d069f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'eta': 0.75,\n",
    "         'max_depth': 150, # 100 88%\n",
    "         'n_estimators' : 500, # 100 88%\n",
    "         'objective' : 'binary:logitraw',\n",
    "         'reg' : 'logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bed8c24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "xgb_train = xgb.DMatrix(train_x,y_train)\n",
    "xgb_test = xgb.DMatrix(test_x, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ce25d9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:09:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"reg\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:09:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model_cv = xgb.train(param, xgb_train, num_boost_round = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "982de507",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost = xgb_model_cv.predict(xgb_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "76598603",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost = np.where(np.array(predict_xgboost) > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dcb5f7",
   "metadata": {},
   "source": [
    "# TF-IDF and SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cd945ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "549d2adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "Tfidf_vect = TfidfVectorizer(tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True))\n",
    "Tfidf_vect.fit(X_train, y_train)\n",
    "Train_X_Tfidf = Tfidf_vect.transform(X_train)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3f333709",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b23997f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(gamma='auto', kernel='linear')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM.fit(Train_X_Tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee20814d",
   "metadata": {},
   "source": [
    "# TF-IDF and XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43333808",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer(tokenizer= lambda x:token_all_proccesing(x, remove_stop_words= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd79428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vnvof\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:516: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(tokenizer=<function <lambda> at 0x000001DAEA490550>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.fit(data['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a599caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'eta': 0.75,\n",
    "         'max_depth': 150, # 100 88%\n",
    "         'n_estimators' : 500, # 100 88%\n",
    "         'objective' : 'binary:logitraw',\n",
    "         'reg' : 'logistic'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac4e30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_tf = tf_idf.transform(X_train)\n",
    "test_x_tf = tf_idf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9aef1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train_tf_idf = xgb.DMatrix(train_x_tf,y_train)\n",
    "xgb_test_tf_idf = xgb.DMatrix(test_x_tf, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "98400eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:15:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"n_estimators\", \"reg\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:15:09] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.4.0, the default evaluation metric used with the objective 'binary:logitraw' was changed from 'auc' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb_model_tf_idf = xgb.train(param, xgb_train_tf_idf, num_boost_round = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca606b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost_tf_idf = xgb_model_tf_idf.predict(xgb_test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d2b4ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_xgboost_tf_idf = np.where(np.array(predict_xgboost_tf_idf) > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "067e7cf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_xgboost_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f55582",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6a6afe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bfb75fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + CountVectorzer Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.87      0.90      5686\n",
      "           1       0.85      0.93      0.89      4796\n",
      "\n",
      "    accuracy                           0.89     10482\n",
      "   macro avg       0.89      0.90      0.89     10482\n",
      "weighted avg       0.90      0.89      0.89     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Logistic Regression + CountVectorzer Metrics:')\n",
    "print (classification_report(predict_CV, y_test))\n",
    "## 25/25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b567979e",
   "metadata": {},
   "source": [
    "## metrics (accuracy) increases by ~ 5%\n",
    "with 25k/25k TRC and 5k/5k RLTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0cb2b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression + TF-IDF Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.89      5607\n",
      "           1       0.86      0.92      0.89      4875\n",
      "\n",
      "    accuracy                           0.89     10482\n",
      "   macro avg       0.89      0.89      0.89     10482\n",
      "weighted avg       0.89      0.89      0.89     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Logistic Regression + TF-IDF Metrics:')\n",
    "print (classification_report(predict_tfidf, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "533d1125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost + CountVectorzer Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.81      0.86      6000\n",
      "           1       0.78      0.90      0.84      4482\n",
      "\n",
      "    accuracy                           0.85     10482\n",
      "   macro avg       0.85      0.86      0.85     10482\n",
      "weighted avg       0.86      0.85      0.85     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('XGBoost + CountVectorzer Metrics:')\n",
    "print (classification_report(predict_xgboost, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d08c6194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost + TF-IDF Metrics:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.85      0.89      5832\n",
      "           1       0.83      0.93      0.87      4650\n",
      "\n",
      "    accuracy                           0.88     10482\n",
      "   macro avg       0.88      0.89      0.88     10482\n",
      "weighted avg       0.89      0.88      0.88     10482\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('XGBoost + TF-IDF Metrics:')\n",
    "print (classification_report(predict_xgboost_tf_idf, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
